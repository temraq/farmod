# # Подготовка данных PubMed QA
# Загрузка, очистка и индексация медицинских данных

# %%
from datasets import load_dataset
from src.data_utils import clean_text, chunk_text
import matplotlib.pyplot as plt

# Загрузка данных
dataset = load_dataset("pubmed_qa", "pqa_labeled")
train_data = dataset['train']

# Анализ данных
print(f"Примеров в тренировочном наборе: {len(train_data)}")
print(f"Ключи: {list(train_data.features.keys())}")

# Пример очистки текста
example = train_data[0]
context = " ".join(example['context'])
cleaned = clean_text(context)
print(f"\nОригинальный текст:\n{context[:200]}...")
print(f"\nОчищенный текст:\n{cleaned[:200]}...")

# Чанкирование
chunks = chunk_text(cleaned, chunk_size=200, overlap=20)
print(f"\nРазбито на {len(chunks)} чанков:")
for i, chunk in enumerate(chunks[:3]):
    print(f"Чанк {i+1}: {chunk[:100]}...")

# Визуализация распределения длин ответов
answer_lengths = [len(ans.split()) for ans in train_data['long_answer']]
plt.hist(answer_lengths, bins=50)
plt.title('Распределение длины ответов')
plt.xlabel('Количество слов')
plt.ylabel('Частота')
plt.show()